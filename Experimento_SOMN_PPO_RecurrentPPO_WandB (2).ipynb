{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iweVuOfcxMBh"
      },
      "source": [
        "# **Experimento Self-Optimising Manufacturing in Network (SOMN)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYRORtcwx92h"
      },
      "source": [
        "## **Instalando WandB**\n",
        "[https://wandb.ai/](https://wandb.ai/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ldvChodvwhpx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "178dea9c-3851-4508-e764-3d826a32dc0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.3/203.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -Uq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkv1OqMAyY_J"
      },
      "source": [
        "**Fazendo login usando minha Api Key**: 1e1748bda3d3df311c1f2248a3dc5d4efea0e752"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TYyWNkZ2yPmy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "477aea2d-2a7f-447a-9ed0-8bf72cbc0d3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BZLZF-vJBJTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34067778-2334-44de-c9c7-c3f50dc59118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfredericmferreira\u001b[0m (\u001b[33mlacmor\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# tentando login\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyJBxWRoGofl"
      },
      "source": [
        "## **Instalando Stable-Baselines3**\n",
        "[https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zQ2qQnaYy8EZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff4efe92-d0ac-4149-f5c4-a1965ea76366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.5/177.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\" -Uq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/Stable-Baselines-Team/stable-baselines3-contrib -Uq"
      ],
      "metadata": {
        "id": "QFhHq2aGzY_1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268420f2-85e7-4807-9bbd-56b52dc18ff3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sb3-contrib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWVAjzYjNLW8"
      },
      "source": [
        "## **Agente Proximal Policy Optimization (PPO)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-d1ROkMVwfV"
      },
      "source": [
        "### **Classe OnPolicyAlgorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c5FM0XwEVw48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9b5a811-a223-428e-9c02-cdb398803ddc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "import sys\n",
        "import time\n",
        "from typing import Any, Dict, List, Optional, Tuple, Type, TypeVar, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from gymnasium import spaces\n",
        "\n",
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "from stable_baselines3.common.buffers import DictRolloutBuffer, RolloutBuffer\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "from stable_baselines3.common.policies import ActorCriticPolicy\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
        "from stable_baselines3.common.utils import obs_as_tensor, safe_mean\n",
        "from stable_baselines3.common.vec_env import VecEnv\n",
        "\n",
        "SelfOnPolicyAlgorithm = TypeVar(\"SelfOnPolicyAlgorithm\", bound=\"OnPolicyAlgorithm\")\n",
        "\n",
        "\n",
        "class OnPolicyAlgorithm(BaseAlgorithm):\n",
        "    \"\"\"\n",
        "    The base for On-Policy algorithms (ex: A2C/PPO).\n",
        "\n",
        "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
        "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
        "    :param learning_rate: The learning rate, it can be a function\n",
        "        of the current progress remaining (from 1 to 0)\n",
        "    :param n_steps: The number of steps to run for each environment per update\n",
        "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
        "    :param gamma: Discount factor\n",
        "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator.\n",
        "        Equivalent to classic advantage when set to 1.\n",
        "    :param ent_coef: Entropy coefficient for the loss calculation\n",
        "    :param vf_coef: Value function coefficient for the loss calculation\n",
        "    :param max_grad_norm: The maximum value for the gradient clipping\n",
        "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
        "        instead of action noise exploration (default: False)\n",
        "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
        "        Default: -1 (only sample at the beginning of the rollout)\n",
        "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
        "        the reported success rate, mean episode length, and mean reward over\n",
        "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
        "    :param monitor_wrapper: When creating an environment, whether to wrap it\n",
        "        or not in a Monitor wrapper.\n",
        "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
        "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
        "        debug messages\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
        "        Setting it to auto, the code will be run on the GPU if possible.\n",
        "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
        "    :param supported_action_spaces: The action spaces supported by the algorithm.\n",
        "    \"\"\"\n",
        "\n",
        "    rollout_buffer: RolloutBuffer\n",
        "    policy: ActorCriticPolicy\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Union[str, Type[ActorCriticPolicy]],\n",
        "        env: Union[GymEnv, str],\n",
        "        learning_rate: Union[float, Schedule],\n",
        "        n_steps: int,\n",
        "        gamma: float,\n",
        "        gae_lambda: float,\n",
        "        ent_coef: float,\n",
        "        vf_coef: float,\n",
        "        max_grad_norm: float,\n",
        "        use_sde: bool,\n",
        "        sde_sample_freq: int,\n",
        "        stats_window_size: int = 100,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        monitor_wrapper: bool = True,\n",
        "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        verbose: int = 0,\n",
        "        seed: Optional[int] = None,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        _init_setup_model: bool = True,\n",
        "        supported_action_spaces: Optional[Tuple[Type[spaces.Space], ...]] = None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            policy=policy,\n",
        "            env=env,\n",
        "            learning_rate=learning_rate,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            use_sde=use_sde,\n",
        "            sde_sample_freq=sde_sample_freq,\n",
        "            support_multi_env=True,\n",
        "            seed=seed,\n",
        "            stats_window_size=stats_window_size,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            supported_action_spaces=supported_action_spaces,\n",
        "        )\n",
        "        #self.acoes\n",
        "        self.n_steps = n_steps\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.ent_coef = ent_coef\n",
        "        self.vf_coef = vf_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        self._setup_lr_schedule()\n",
        "        self.set_random_seed(self.seed)\n",
        "\n",
        "        buffer_cls = DictRolloutBuffer if isinstance(self.observation_space, spaces.Dict) else RolloutBuffer\n",
        "\n",
        "        self.rollout_buffer = buffer_cls(\n",
        "            self.n_steps,\n",
        "            self.observation_space,\n",
        "            self.action_space,\n",
        "            device=self.device,\n",
        "            gamma=self.gamma,\n",
        "            gae_lambda=self.gae_lambda,\n",
        "            n_envs=self.n_envs,\n",
        "        )\n",
        "        # pytype:disable=not-instantiable\n",
        "        self.policy = self.policy_class(  # type: ignore[assignment]\n",
        "            self.observation_space, self.action_space, self.lr_schedule, use_sde=self.use_sde, **self.policy_kwargs\n",
        "        )\n",
        "        # pytype:enable=not-instantiable\n",
        "        self.policy = self.policy.to(self.device)\n",
        "\n",
        "    def collect_rollouts(\n",
        "        self,\n",
        "        env: VecEnv,\n",
        "        callback: BaseCallback,\n",
        "        rollout_buffer: RolloutBuffer,\n",
        "        n_rollout_steps: int,\n",
        "    ) -> bool:\n",
        "        \"\"\"\n",
        "        Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
        "        The term rollout here refers to the model-free notion and should not\n",
        "        be used with the concept of rollout used in model-based RL or planning.\n",
        "\n",
        "        :param env: The training environment\n",
        "        :param callback: Callback that will be called at each step\n",
        "            (and at the beginning and end of the rollout)\n",
        "        :param rollout_buffer: Buffer to fill with rollouts\n",
        "        :param n_rollout_steps: Number of experiences to collect per environment\n",
        "        :return: True if function returned with at least `n_rollout_steps`\n",
        "            collected, False if callback terminated rollout prematurely.\n",
        "        \"\"\"\n",
        "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
        "        # Switch to eval mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(False)\n",
        "\n",
        "        n_steps = 0\n",
        "        rollout_buffer.reset()\n",
        "        # Sample new weights for the state dependent exploration\n",
        "        if self.use_sde:\n",
        "            self.policy.reset_noise(env.num_envs)\n",
        "\n",
        "        callback.on_rollout_start()\n",
        "\n",
        "        while n_steps < n_rollout_steps:\n",
        "            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
        "                # Sample a new noise matrix\n",
        "                self.policy.reset_noise(env.num_envs)\n",
        "\n",
        "            with th.no_grad():\n",
        "                # Convert to pytorch tensor or to TensorDict\n",
        "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
        "                actions, values, log_probs = self.policy(obs_tensor)\n",
        "            actions = actions.cpu().numpy()\n",
        "\n",
        "            # Rescale and perform action\n",
        "            clipped_actions = actions\n",
        "            # Clip the actions to avoid out of bound error\n",
        "            if isinstance(self.action_space, spaces.Box):\n",
        "                clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
        "\n",
        "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
        "\n",
        "            self.num_timesteps += env.num_envs\n",
        "\n",
        "            # Give access to local variables\n",
        "            callback.update_locals(locals())\n",
        "            if callback.on_step() is False:\n",
        "                return False\n",
        "\n",
        "            self._update_info_buffer(infos)\n",
        "            n_steps += 1\n",
        "\n",
        "            if isinstance(self.action_space, spaces.Discrete):\n",
        "                # Reshape in case of discrete action\n",
        "                actions = actions.reshape(-1, 1)\n",
        "\n",
        "            # Handle timeout by bootstraping with value function\n",
        "            # see GitHub issue #633\n",
        "            for idx, done in enumerate(dones):\n",
        "                if (\n",
        "                    done\n",
        "                    and infos[idx].get(\"terminal_observation\") is not None\n",
        "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
        "                ):\n",
        "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
        "                    with th.no_grad():\n",
        "                        terminal_value = self.policy.predict_values(terminal_obs)[0]  # type: ignore[arg-type]\n",
        "                    rewards[idx] += self.gamma * terminal_value\n",
        "\n",
        "            rollout_buffer.add(\n",
        "                self._last_obs,  # type: ignore[arg-type]\n",
        "                actions,\n",
        "                rewards,\n",
        "                self._last_episode_starts,  # type: ignore[arg-type]\n",
        "                values,\n",
        "                log_probs,\n",
        "            )\n",
        "            self._last_obs = new_obs  # type: ignore[assignment]\n",
        "            self._last_episode_starts = dones\n",
        "\n",
        "        with th.no_grad():\n",
        "            # Compute value for the last timestep\n",
        "            values = self.policy.predict_values(obs_as_tensor(new_obs, self.device))  # type: ignore[arg-type]\n",
        "\n",
        "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
        "\n",
        "        callback.on_rollout_end()\n",
        "\n",
        "        return True\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"\n",
        "        Consume current rollout data and update policy parameters.\n",
        "        Implemented by individual algorithms.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def learn(\n",
        "        self: SelfOnPolicyAlgorithm,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 1,\n",
        "        tb_log_name: str = \"OnPolicyAlgorithm\",\n",
        "        reset_num_timesteps: bool = True,\n",
        "        progress_bar: bool = False,\n",
        "    ) -> SelfOnPolicyAlgorithm:\n",
        "        iteration = 0\n",
        "\n",
        "        total_timesteps, callback = self._setup_learn(\n",
        "            total_timesteps,\n",
        "            callback,\n",
        "            reset_num_timesteps,\n",
        "            tb_log_name,\n",
        "            progress_bar,\n",
        "        )\n",
        "\n",
        "        callback.on_training_start(locals(), globals())\n",
        "\n",
        "        assert self.env is not None\n",
        "\n",
        "        #todas_acoes = []    # (by_frederic)\n",
        "\n",
        "        while self.num_timesteps < total_timesteps:\n",
        "            continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
        "\n",
        "            if continue_training is False:\n",
        "                break\n",
        "\n",
        "            iteration += 1\n",
        "            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n",
        "\n",
        "            # Display training infos\n",
        "            if log_interval is not None and iteration % log_interval == 0:\n",
        "                assert self.ep_info_buffer is not None\n",
        "                time_elapsed = max((time.time_ns() - self.start_time) / 1e9, sys.float_info.epsilon)\n",
        "                fps = int((self.num_timesteps - self._num_timesteps_at_start) / time_elapsed)\n",
        "                self.logger.record(\"time/iterations\", iteration, exclude=\"tensorboard\")\n",
        "                if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
        "                    self.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
        "                    self.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
        "                    # --------- WandB Log ----------- #\n",
        "                    wandb.log({\"mean_reward_test\": safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),'timesteps': self.num_timesteps})\n",
        "                    wandb.log({\"ep_len_mean\": safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]),'timesteps': self.num_timesteps})\n",
        "                self.logger.record(\"time/fps\", fps)\n",
        "                self.logger.record(\"time/time_elapsed\", int(time_elapsed), exclude=\"tensorboard\")\n",
        "                self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
        "                self.logger.dump(step=self.num_timesteps)\n",
        "                # --------- WandB Log ----------- #\n",
        "                #wandb.log({\"total_timesteps\": self.num_timesteps})\n",
        "\n",
        "                # --------- WandB Log ----------- #\n",
        "                #wandb.log({'reward': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]), 'timesteps': self.num_timesteps})\n",
        "\n",
        "                # Set up data to log in custom charts\n",
        "                #global acoes\n",
        "                #todas_acoes.append([iteration, acoes])\n",
        "                \n",
        "\n",
        "\n",
        "            #del globals()['acoes']\n",
        "            self.train()\n",
        "\n",
        "        \n",
        "        # Create a table with the columns to plot\n",
        "        #table = wandb.Table(data=todas_acoes, columns=[\"step\", \"acao\"])\n",
        "\n",
        "        # Use the table to populate various custom charts\n",
        "        #line_plot = wandb.plot.line(table, x='step', y='height', title='Line Plot')\n",
        "        #histogram = wandb.plot.histogram(table, value='height', title='Histogram')\n",
        "        #scatter = wandb.plot.scatter(table, x='step', y='acao', title='Acões escolhidas')\n",
        "        \n",
        "        # Log custom tables, which will show up in customizable charts in the UI\n",
        "        #wandb.log({#'line_1': line_plot, \n",
        "                    #'histogram_1': histogram, \n",
        "                    #'scatter_1': scatter})\n",
        "\n",
        "        callback.on_training_end()\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _get_torch_save_params(self) -> Tuple[List[str], List[str]]:\n",
        "        state_dicts = [\"policy\", \"policy.optimizer\"]\n",
        "\n",
        "        return state_dicts, []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaLemITnVkVc"
      },
      "source": [
        "### **Classe PPO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "7eteuJwXNXr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52eeab4-6093-4a2e-98a2-3143936adee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "import warnings\n",
        "from typing import Any, Dict, Optional, Type, TypeVar, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from gymnasium import spaces\n",
        "from torch.nn import functional as F\n",
        "\n",
        "#from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
        "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
        "from stable_baselines3.common.utils import explained_variance, get_schedule_fn\n",
        "\n",
        "SelfPPO = TypeVar(\"SelfPPO\", bound=\"PPO\")\n",
        "\n",
        "\n",
        "class PPO(OnPolicyAlgorithm):\n",
        "    \"\"\"\n",
        "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
        "\n",
        "    Paper: https://arxiv.org/abs/1707.06347\n",
        "    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
        "    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
        "    Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
        "\n",
        "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
        "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
        "    :param learning_rate: The learning rate, it can be a function\n",
        "        of the current progress remaining (from 1 to 0)\n",
        "    :param n_steps: The number of steps to run for each environment per update\n",
        "        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
        "        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
        "        See https://github.com/pytorch/pytorch/issues/29372\n",
        "    :param batch_size: Minibatch size\n",
        "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
        "    :param gamma: Discount factor\n",
        "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
        "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
        "        remaining (from 1 to 0).\n",
        "    :param clip_range_vf: Clipping parameter for the value function,\n",
        "        it can be a function of the current progress remaining (from 1 to 0).\n",
        "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
        "        no clipping will be done on the value function.\n",
        "        IMPORTANT: this clipping depends on the reward scaling.\n",
        "    :param normalize_advantage: Whether to normalize or not the advantage\n",
        "    :param ent_coef: Entropy coefficient for the loss calculation\n",
        "    :param vf_coef: Value function coefficient for the loss calculation\n",
        "    :param max_grad_norm: The maximum value for the gradient clipping\n",
        "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
        "        instead of action noise exploration (default: False)\n",
        "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
        "        Default: -1 (only sample at the beginning of the rollout)\n",
        "    :param target_kl: Limit the KL divergence between updates,\n",
        "        because the clipping is not enough to prevent large update\n",
        "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
        "        By default, there is no limit on the kl div.\n",
        "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
        "        the reported success rate, mean episode length, and mean reward over\n",
        "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
        "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
        "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
        "        debug messages\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
        "        Setting it to auto, the code will be run on the GPU if possible.\n",
        "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
        "    \"\"\"\n",
        "\n",
        "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
        "        \"MlpPolicy\": ActorCriticPolicy,\n",
        "        \"CnnPolicy\": ActorCriticCnnPolicy,\n",
        "        \"MultiInputPolicy\": MultiInputActorCriticPolicy,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Union[str, Type[ActorCriticPolicy]],\n",
        "        env: Union[GymEnv, str],\n",
        "        learning_rate: Union[float, Schedule] = 3e-4,\n",
        "        n_steps: int = 2048,\n",
        "        batch_size: int = 64,\n",
        "        n_epochs: int = 10,\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        clip_range: Union[float, Schedule] = 0.2,\n",
        "        clip_range_vf: Union[None, float, Schedule] = None,\n",
        "        normalize_advantage: bool = True,\n",
        "        ent_coef: float = 0.0,\n",
        "        vf_coef: float = 0.5,\n",
        "        max_grad_norm: float = 0.5,\n",
        "        use_sde: bool = False,\n",
        "        sde_sample_freq: int = -1,\n",
        "        target_kl: Optional[float] = None,\n",
        "        stats_window_size: int = 100,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        verbose: int = 0,\n",
        "        seed: Optional[int] = None,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        _init_setup_model: bool = True,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            policy,\n",
        "            env,\n",
        "            learning_rate=learning_rate,\n",
        "            n_steps=n_steps,\n",
        "            gamma=gamma,\n",
        "            gae_lambda=gae_lambda,\n",
        "            ent_coef=ent_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            use_sde=use_sde,\n",
        "            sde_sample_freq=sde_sample_freq,\n",
        "            stats_window_size=stats_window_size,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            seed=seed,\n",
        "            _init_setup_model=False,\n",
        "            supported_action_spaces=(\n",
        "                spaces.Box,\n",
        "                spaces.Discrete,\n",
        "                spaces.MultiDiscrete,\n",
        "                spaces.MultiBinary,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Sanity check, otherwise it will lead to noisy gradient and NaN\n",
        "        # because of the advantage normalization\n",
        "        if normalize_advantage:\n",
        "            assert (\n",
        "                batch_size > 1\n",
        "            ), \"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\n",
        "\n",
        "        if self.env is not None:\n",
        "            # Check that `n_steps * n_envs > 1` to avoid NaN\n",
        "            # when doing advantage normalization\n",
        "            buffer_size = self.env.num_envs * self.n_steps\n",
        "            assert buffer_size > 1 or (\n",
        "                not normalize_advantage\n",
        "            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n",
        "            # Check that the rollout buffer size is a multiple of the mini-batch size\n",
        "            untruncated_batches = buffer_size // batch_size\n",
        "            if buffer_size % batch_size > 0:\n",
        "                warnings.warn(\n",
        "                    f\"You have specified a mini-batch size of {batch_size},\"\n",
        "                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n",
        "                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n",
        "                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n",
        "                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n",
        "                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n",
        "                )\n",
        "        #self.acoes = [] # (by_frederic)\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.clip_range = clip_range\n",
        "        self.clip_range_vf = clip_range_vf\n",
        "        self.normalize_advantage = normalize_advantage\n",
        "        self.target_kl = target_kl\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        super()._setup_model()\n",
        "\n",
        "        # Initialize schedules for policy/value clipping\n",
        "        self.clip_range = get_schedule_fn(self.clip_range)\n",
        "        if self.clip_range_vf is not None:\n",
        "            if isinstance(self.clip_range_vf, (float, int)):\n",
        "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, \" \"pass `None` to deactivate vf clipping\"\n",
        "\n",
        "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"\n",
        "        Update policy using the currently gathered rollout buffer.\n",
        "        \"\"\"\n",
        "        # Switch to train mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(True)\n",
        "        # Update optimizer learning rate\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "        # Compute current clip range\n",
        "        clip_range = self.clip_range(self._current_progress_remaining)  # type: ignore[operator]\n",
        "        # Optional: clip range for the value function\n",
        "        if self.clip_range_vf is not None:\n",
        "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)  # type: ignore[operator]\n",
        "\n",
        "        # acoes para avaliacao (by_frederic)\n",
        "        #global acoes\n",
        "\n",
        "        entropy_losses = []\n",
        "        pg_losses, value_losses = [], []\n",
        "        clip_fractions = []\n",
        "\n",
        "        continue_training = True\n",
        "        # train for n_epochs epochs\n",
        "        for epoch in range(self.n_epochs):\n",
        "            approx_kl_divs = []\n",
        "            # Do a complete pass on the rollout buffer\n",
        "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
        "                actions = rollout_data.actions\n",
        "                \n",
        "                \n",
        "                if isinstance(self.action_space, spaces.Discrete):\n",
        "                    # Convert discrete action from float to long\n",
        "                    actions = rollout_data.actions.long().flatten()\n",
        "                    \n",
        "                # (by_frederic)\n",
        "                #self.acoes.append(actions.item())\n",
        "\n",
        "                # Re-sample the noise matrix because the log_std has changed\n",
        "                if self.use_sde:\n",
        "                    self.policy.reset_noise(self.batch_size)\n",
        "\n",
        "                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
        "                values = values.flatten()\n",
        "                # Normalize advantage\n",
        "                advantages = rollout_data.advantages\n",
        "                # Normalization does not make sense if mini batchsize == 1, see GH issue #325\n",
        "                if self.normalize_advantage and len(advantages) > 1:\n",
        "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "                # ratio between old and new policy, should be one at the first iteration\n",
        "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
        "\n",
        "                # clipped surrogate loss\n",
        "                policy_loss_1 = advantages * ratio\n",
        "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
        "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
        "\n",
        "                # Logging\n",
        "                pg_losses.append(policy_loss.item())\n",
        "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
        "                clip_fractions.append(clip_fraction)\n",
        "\n",
        "                if self.clip_range_vf is None:\n",
        "                    # No clipping\n",
        "                    values_pred = values\n",
        "                else:\n",
        "                    # Clip the difference between old and new value\n",
        "                    # NOTE: this depends on the reward scaling\n",
        "                    values_pred = rollout_data.old_values + th.clamp(\n",
        "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
        "                    )\n",
        "                # Value loss using the TD(gae_lambda) target\n",
        "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
        "                value_losses.append(value_loss.item())\n",
        "\n",
        "                # acoes para avaliacao (by_frederic)\n",
        "                #acoes.append(actions.item())\n",
        "\n",
        "                # Entropy loss favor exploration\n",
        "                if entropy is None:\n",
        "                    # Approximate entropy when no analytical form\n",
        "                    entropy_loss = -th.mean(-log_prob)\n",
        "                else:\n",
        "                    entropy_loss = -th.mean(entropy)\n",
        "\n",
        "                entropy_losses.append(entropy_loss.item())\n",
        "\n",
        "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
        "\n",
        "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
        "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
        "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
        "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
        "                with th.no_grad():\n",
        "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
        "                    approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
        "                    approx_kl_divs.append(approx_kl_div)\n",
        "\n",
        "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
        "                    continue_training = False\n",
        "                    if self.verbose >= 1:\n",
        "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
        "                    break\n",
        "\n",
        "                # Optimization step\n",
        "                self.policy.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                # Clip grad norm\n",
        "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "                self.policy.optimizer.step()\n",
        "\n",
        "            self._n_updates += 1\n",
        "            if not continue_training:\n",
        "                break\n",
        "\n",
        "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
        "\n",
        "        # Logs\n",
        "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
        "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
        "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
        "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
        "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
        "        self.logger.record(\"train/loss\", loss.item())\n",
        "        self.logger.record(\"train/explained_variance\", explained_var)\n",
        "        if hasattr(self.policy, \"log_std\"):\n",
        "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
        "\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        self.logger.record(\"train/clip_range\", clip_range)\n",
        "        if self.clip_range_vf is not None:\n",
        "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
        "\n",
        "        # --------- WandB Log ----------- #\n",
        "        wandb.log({'entropy_loss': np.mean(entropy_losses), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'policy_gradient_loss': np.mean(pg_losses), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'value_loss': np.mean(value_losses), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'approx_kl': np.mean(approx_kl_divs), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'clip_fraction': np.mean(clip_fractions), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'loss': loss.item(), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'explained_variance': explained_var, 'timesteps': self.num_timesteps})\n",
        "\n",
        "        # Customizar um grafico para as acoes no tempo\n",
        "        acoes = actions.tolist()\n",
        "        \n",
        "        wandb.log({'acoes':  np.mean(acoes), \n",
        "                   'timesteps': self.num_timesteps, \n",
        "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
        "                   'value_loss': np.mean(value_losses), \n",
        "                   'loss': loss.item()\n",
        "                   }\n",
        "        )\n",
        "        \n",
        "        wandb.log({'Yard': (Yard.cont/Yard.space)*100, \n",
        "                   'timesteps': self.num_timesteps, \n",
        "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
        "                   'value_loss': np.mean(value_losses), \n",
        "                   'loss': loss.item()\n",
        "                   }\n",
        "        )\n",
        "        # --------- WandB Log ----------- #\n",
        "\n",
        "    def learn(\n",
        "        self: SelfPPO,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 1,\n",
        "        tb_log_name: str = \"PPO\",\n",
        "        reset_num_timesteps: bool = True,\n",
        "        progress_bar: bool = False,\n",
        "    ) -> SelfPPO:\n",
        "        return super().learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=callback,\n",
        "            log_interval=log_interval,\n",
        "            tb_log_name=tb_log_name,\n",
        "            reset_num_timesteps=reset_num_timesteps,\n",
        "            progress_bar=progress_bar,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Classe RecurrentPPO**"
      ],
      "metadata": {
        "id": "DSK2CdgeTt3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "import sys\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from typing import Any, Dict, Optional, Type, TypeVar, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common.buffers import RolloutBuffer\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "#from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
        "from stable_baselines3.common.policies import BasePolicy\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
        "from stable_baselines3.common.utils import explained_variance, get_schedule_fn, obs_as_tensor, safe_mean\n",
        "from stable_baselines3.common.vec_env import VecEnv\n",
        "\n",
        "from sb3_contrib.common.recurrent.buffers import RecurrentDictRolloutBuffer, RecurrentRolloutBuffer\n",
        "from sb3_contrib.common.recurrent.policies import RecurrentActorCriticPolicy\n",
        "from sb3_contrib.common.recurrent.type_aliases import RNNStates\n",
        "from sb3_contrib.ppo_recurrent.policies import CnnLstmPolicy, MlpLstmPolicy, MultiInputLstmPolicy\n",
        "\n",
        "SelfRecurrentPPO = TypeVar(\"SelfRecurrentPPO\", bound=\"RecurrentPPO\")\n",
        "\n",
        "\n",
        "class RecurrentPPO(OnPolicyAlgorithm):\n",
        "    \"\"\"\n",
        "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
        "    with support for recurrent policies (LSTM).\n",
        "\n",
        "    Based on the original Stable Baselines 3 implementation.\n",
        "\n",
        "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
        "\n",
        "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
        "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
        "    :param learning_rate: The learning rate, it can be a function\n",
        "        of the current progress remaining (from 1 to 0)\n",
        "    :param n_steps: The number of steps to run for each environment per update\n",
        "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
        "    :param batch_size: Minibatch size\n",
        "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
        "    :param gamma: Discount factor\n",
        "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
        "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
        "        remaining (from 1 to 0).\n",
        "    :param clip_range_vf: Clipping parameter for the value function,\n",
        "        it can be a function of the current progress remaining (from 1 to 0).\n",
        "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
        "        no clipping will be done on the value function.\n",
        "        IMPORTANT: this clipping depends on the reward scaling.\n",
        "    :param normalize_advantage: Whether to normalize or not the advantage\n",
        "    :param ent_coef: Entropy coefficient for the loss calculation\n",
        "    :param vf_coef: Value function coefficient for the loss calculation\n",
        "    :param max_grad_norm: The maximum value for the gradient clipping\n",
        "    :param target_kl: Limit the KL divergence between updates,\n",
        "        because the clipping is not enough to prevent large update\n",
        "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
        "        By default, there is no limit on the kl div.\n",
        "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
        "        the reported success rate, mean episode length, and mean reward over\n",
        "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
        "    :param policy_kwargs: additional arguments to be passed to the policy on creation\n",
        "    :param verbose: the verbosity level: 0 no output, 1 info, 2 debug\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
        "        Setting it to auto, the code will be run on the GPU if possible.\n",
        "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
        "    \"\"\"\n",
        "\n",
        "    policy_aliases: Dict[str, Type[BasePolicy]] = {\n",
        "        \"MlpLstmPolicy\": MlpLstmPolicy,\n",
        "        \"CnnLstmPolicy\": CnnLstmPolicy,\n",
        "        \"MultiInputLstmPolicy\": MultiInputLstmPolicy,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Union[str, Type[RecurrentActorCriticPolicy]],\n",
        "        env: Union[GymEnv, str],\n",
        "        learning_rate: Union[float, Schedule] = 3e-4,\n",
        "        n_steps: int = 128,\n",
        "        batch_size: Optional[int] = 128,\n",
        "        n_epochs: int = 10,\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        clip_range: Union[float, Schedule] = 0.2,\n",
        "        clip_range_vf: Union[None, float, Schedule] = None,\n",
        "        normalize_advantage: bool = True,\n",
        "        ent_coef: float = 0.0,\n",
        "        vf_coef: float = 0.5,\n",
        "        max_grad_norm: float = 0.5,\n",
        "        use_sde: bool = False,\n",
        "        sde_sample_freq: int = -1,\n",
        "        target_kl: Optional[float] = None,\n",
        "        stats_window_size: int = 100,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        policy_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        verbose: int = 0,\n",
        "        seed: Optional[int] = None,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        _init_setup_model: bool = True,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            policy,\n",
        "            env,\n",
        "            learning_rate=learning_rate,\n",
        "            n_steps=n_steps,\n",
        "            gamma=gamma,\n",
        "            gae_lambda=gae_lambda,\n",
        "            ent_coef=ent_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            use_sde=use_sde,\n",
        "            sde_sample_freq=sde_sample_freq,\n",
        "            stats_window_size=stats_window_size,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=verbose,\n",
        "            seed=seed,\n",
        "            device=device,\n",
        "            _init_setup_model=False,\n",
        "            supported_action_spaces=(\n",
        "                spaces.Box,\n",
        "                spaces.Discrete,\n",
        "                spaces.MultiDiscrete,\n",
        "                spaces.MultiBinary,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.clip_range = clip_range\n",
        "        self.clip_range_vf = clip_range_vf\n",
        "        self.normalize_advantage = normalize_advantage\n",
        "        self.target_kl = target_kl\n",
        "        self._last_lstm_states = None\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        self._setup_lr_schedule()\n",
        "        self.set_random_seed(self.seed)\n",
        "\n",
        "        buffer_cls = RecurrentDictRolloutBuffer if isinstance(self.observation_space, spaces.Dict) else RecurrentRolloutBuffer\n",
        "\n",
        "        self.policy = self.policy_class(\n",
        "            self.observation_space,\n",
        "            self.action_space,\n",
        "            self.lr_schedule,\n",
        "            use_sde=self.use_sde,\n",
        "            **self.policy_kwargs,  # pytype:disable=not-instantiable\n",
        "        )\n",
        "        self.policy = self.policy.to(self.device)\n",
        "\n",
        "        # We assume that LSTM for the actor and the critic\n",
        "        # have the same architecture\n",
        "        lstm = self.policy.lstm_actor\n",
        "\n",
        "        if not isinstance(self.policy, RecurrentActorCriticPolicy):\n",
        "            raise ValueError(\"Policy must subclass RecurrentActorCriticPolicy\")\n",
        "\n",
        "        single_hidden_state_shape = (lstm.num_layers, self.n_envs, lstm.hidden_size)\n",
        "        # hidden and cell states for actor and critic\n",
        "        self._last_lstm_states = RNNStates(\n",
        "            (\n",
        "                th.zeros(single_hidden_state_shape, device=self.device),\n",
        "                th.zeros(single_hidden_state_shape, device=self.device),\n",
        "            ),\n",
        "            (\n",
        "                th.zeros(single_hidden_state_shape, device=self.device),\n",
        "                th.zeros(single_hidden_state_shape, device=self.device),\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        hidden_state_buffer_shape = (self.n_steps, lstm.num_layers, self.n_envs, lstm.hidden_size)\n",
        "\n",
        "        self.rollout_buffer = buffer_cls(\n",
        "            self.n_steps,\n",
        "            self.observation_space,\n",
        "            self.action_space,\n",
        "            hidden_state_buffer_shape,\n",
        "            self.device,\n",
        "            gamma=self.gamma,\n",
        "            gae_lambda=self.gae_lambda,\n",
        "            n_envs=self.n_envs,\n",
        "        )\n",
        "\n",
        "        # Initialize schedules for policy/value clipping\n",
        "        self.clip_range = get_schedule_fn(self.clip_range)\n",
        "        if self.clip_range_vf is not None:\n",
        "            if isinstance(self.clip_range_vf, (float, int)):\n",
        "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, pass `None` to deactivate vf clipping\"\n",
        "\n",
        "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
        "\n",
        "    def collect_rollouts(\n",
        "        self,\n",
        "        env: VecEnv,\n",
        "        callback: BaseCallback,\n",
        "        rollout_buffer: RolloutBuffer,\n",
        "        n_rollout_steps: int,\n",
        "    ) -> bool:\n",
        "        \"\"\"\n",
        "        Collect experiences using the current policy and fill a ``RolloutBuffer``.\n",
        "        The term rollout here refers to the model-free notion and should not\n",
        "        be used with the concept of rollout used in model-based RL or planning.\n",
        "\n",
        "        :param env: The training environment\n",
        "        :param callback: Callback that will be called at each step\n",
        "            (and at the beginning and end of the rollout)\n",
        "        :param rollout_buffer: Buffer to fill with rollouts\n",
        "        :param n_steps: Number of experiences to collect per environment\n",
        "        :return: True if function returned with at least `n_rollout_steps`\n",
        "            collected, False if callback terminated rollout prematurely.\n",
        "        \"\"\"\n",
        "        assert isinstance(\n",
        "            rollout_buffer, (RecurrentRolloutBuffer, RecurrentDictRolloutBuffer)\n",
        "        ), f\"{rollout_buffer} doesn't support recurrent policy\"\n",
        "\n",
        "        assert self._last_obs is not None, \"No previous observation was provided\"\n",
        "        # Switch to eval mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(False)\n",
        "\n",
        "        n_steps = 0\n",
        "        rollout_buffer.reset()\n",
        "        # Sample new weights for the state dependent exploration\n",
        "        if self.use_sde:\n",
        "            self.policy.reset_noise(env.num_envs)\n",
        "\n",
        "        callback.on_rollout_start()\n",
        "\n",
        "        lstm_states = deepcopy(self._last_lstm_states)\n",
        "\n",
        "        while n_steps < n_rollout_steps:\n",
        "            if self.use_sde and self.sde_sample_freq > 0 and n_steps % self.sde_sample_freq == 0:\n",
        "                # Sample a new noise matrix\n",
        "                self.policy.reset_noise(env.num_envs)\n",
        "\n",
        "            with th.no_grad():\n",
        "                # Convert to pytorch tensor or to TensorDict\n",
        "                obs_tensor = obs_as_tensor(self._last_obs, self.device)\n",
        "                episode_starts = th.tensor(self._last_episode_starts, dtype=th.float32, device=self.device)\n",
        "                actions, values, log_probs, lstm_states = self.policy.forward(obs_tensor, lstm_states, episode_starts)\n",
        "\n",
        "            actions = actions.cpu().numpy()\n",
        "\n",
        "            # Rescale and perform action\n",
        "            clipped_actions = actions\n",
        "            # Clip the actions to avoid out of bound error\n",
        "            if isinstance(self.action_space, spaces.Box):\n",
        "                clipped_actions = np.clip(actions, self.action_space.low, self.action_space.high)\n",
        "\n",
        "            new_obs, rewards, dones, infos = env.step(clipped_actions)\n",
        "\n",
        "            self.num_timesteps += env.num_envs\n",
        "\n",
        "            # Give access to local variables\n",
        "            callback.update_locals(locals())\n",
        "            if callback.on_step() is False:\n",
        "                return False\n",
        "\n",
        "            self._update_info_buffer(infos)\n",
        "            n_steps += 1\n",
        "\n",
        "            if isinstance(self.action_space, spaces.Discrete):\n",
        "                # Reshape in case of discrete action\n",
        "                actions = actions.reshape(-1, 1)\n",
        "\n",
        "            # Handle timeout by bootstraping with value function\n",
        "            # see GitHub issue #633\n",
        "            for idx, done_ in enumerate(dones):\n",
        "                if (\n",
        "                    done_\n",
        "                    and infos[idx].get(\"terminal_observation\") is not None\n",
        "                    and infos[idx].get(\"TimeLimit.truncated\", False)\n",
        "                ):\n",
        "                    terminal_obs = self.policy.obs_to_tensor(infos[idx][\"terminal_observation\"])[0]\n",
        "                    with th.no_grad():\n",
        "                        terminal_lstm_state = (\n",
        "                            lstm_states.vf[0][:, idx : idx + 1, :].contiguous(),\n",
        "                            lstm_states.vf[1][:, idx : idx + 1, :].contiguous(),\n",
        "                        )\n",
        "                        # terminal_lstm_state = None\n",
        "                        episode_starts = th.tensor([False], dtype=th.float32, device=self.device)\n",
        "                        terminal_value = self.policy.predict_values(terminal_obs, terminal_lstm_state, episode_starts)[0]\n",
        "                    rewards[idx] += self.gamma * terminal_value\n",
        "\n",
        "            rollout_buffer.add(\n",
        "                self._last_obs,\n",
        "                actions,\n",
        "                rewards,\n",
        "                self._last_episode_starts,\n",
        "                values,\n",
        "                log_probs,\n",
        "                lstm_states=self._last_lstm_states,\n",
        "            )\n",
        "\n",
        "            self._last_obs = new_obs\n",
        "            self._last_episode_starts = dones\n",
        "            self._last_lstm_states = lstm_states\n",
        "\n",
        "        with th.no_grad():\n",
        "            # Compute value for the last timestep\n",
        "            episode_starts = th.tensor(dones, dtype=th.float32, device=self.device)\n",
        "            values = self.policy.predict_values(obs_as_tensor(new_obs, self.device), lstm_states.vf, episode_starts)\n",
        "\n",
        "        rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)\n",
        "\n",
        "        callback.on_rollout_end()\n",
        "\n",
        "        return True\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"\n",
        "        Update policy using the currently gathered rollout buffer.\n",
        "        \"\"\"\n",
        "        # Switch to train mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(True)\n",
        "        # Update optimizer learning rate\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "        # Compute current clip range\n",
        "        clip_range = self.clip_range(self._current_progress_remaining)\n",
        "        # Optional: clip range for the value function\n",
        "        if self.clip_range_vf is not None:\n",
        "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)\n",
        "\n",
        "        entropy_losses = []\n",
        "        pg_losses, value_losses = [], []\n",
        "        clip_fractions = []\n",
        "\n",
        "        continue_training = True\n",
        "\n",
        "        # train for n_epochs epochs\n",
        "        for epoch in range(self.n_epochs):\n",
        "            approx_kl_divs = []\n",
        "            # Do a complete pass on the rollout buffer\n",
        "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
        "                actions = rollout_data.actions\n",
        "                if isinstance(self.action_space, spaces.Discrete):\n",
        "                    # Convert discrete action from float to long\n",
        "                    actions = rollout_data.actions.long().flatten()\n",
        "\n",
        "                # Convert mask from float to bool\n",
        "                mask = rollout_data.mask > 1e-8\n",
        "\n",
        "                # Re-sample the noise matrix because the log_std has changed\n",
        "                if self.use_sde:\n",
        "                    self.policy.reset_noise(self.batch_size)\n",
        "\n",
        "                values, log_prob, entropy = self.policy.evaluate_actions(\n",
        "                    rollout_data.observations,\n",
        "                    actions,\n",
        "                    rollout_data.lstm_states,\n",
        "                    rollout_data.episode_starts,\n",
        "                )\n",
        "\n",
        "                values = values.flatten()\n",
        "                # Normalize advantage\n",
        "                advantages = rollout_data.advantages\n",
        "                if self.normalize_advantage:\n",
        "                    advantages = (advantages - advantages[mask].mean()) / (advantages[mask].std() + 1e-8)\n",
        "\n",
        "                # ratio between old and new policy, should be one at the first iteration\n",
        "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
        "\n",
        "                # clipped surrogate loss\n",
        "                policy_loss_1 = advantages * ratio\n",
        "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
        "                policy_loss = -th.mean(th.min(policy_loss_1, policy_loss_2)[mask])\n",
        "\n",
        "                # Logging\n",
        "                pg_losses.append(policy_loss.item())\n",
        "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()[mask]).item()\n",
        "                clip_fractions.append(clip_fraction)\n",
        "\n",
        "                if self.clip_range_vf is None:\n",
        "                    # No clipping\n",
        "                    values_pred = values\n",
        "                else:\n",
        "                    # Clip the different between old and new value\n",
        "                    # NOTE: this depends on the reward scaling\n",
        "                    values_pred = rollout_data.old_values + th.clamp(\n",
        "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
        "                    )\n",
        "                # Value loss using the TD(gae_lambda) target\n",
        "                # Mask padded sequences\n",
        "                value_loss = th.mean(((rollout_data.returns - values_pred) ** 2)[mask])\n",
        "\n",
        "                value_losses.append(value_loss.item())\n",
        "\n",
        "                # Entropy loss favor exploration\n",
        "                if entropy is None:\n",
        "                    # Approximate entropy when no analytical form\n",
        "                    entropy_loss = -th.mean(-log_prob[mask])\n",
        "                else:\n",
        "                    entropy_loss = -th.mean(entropy[mask])\n",
        "\n",
        "                entropy_losses.append(entropy_loss.item())\n",
        "\n",
        "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
        "\n",
        "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
        "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
        "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
        "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
        "                with th.no_grad():\n",
        "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
        "                    approx_kl_div = th.mean(((th.exp(log_ratio) - 1) - log_ratio)[mask]).cpu().numpy()\n",
        "                    approx_kl_divs.append(approx_kl_div)\n",
        "\n",
        "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
        "                    continue_training = False\n",
        "                    if self.verbose >= 1:\n",
        "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
        "                    break\n",
        "\n",
        "                # Optimization step\n",
        "                self.policy.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                # Clip grad norm\n",
        "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "                self.policy.optimizer.step()\n",
        "\n",
        "            if not continue_training:\n",
        "                break\n",
        "\n",
        "        self._n_updates += self.n_epochs\n",
        "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
        "\n",
        "        # Logs\n",
        "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
        "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
        "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
        "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
        "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
        "        self.logger.record(\"train/loss\", loss.item())\n",
        "        self.logger.record(\"train/explained_variance\", explained_var)\n",
        "        if hasattr(self.policy, \"log_std\"):\n",
        "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
        "\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        self.logger.record(\"train/clip_range\", clip_range)\n",
        "        if self.clip_range_vf is not None:\n",
        "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
        "\n",
        "        # --------- WandB Log ----------- #\n",
        "        wandb.log({'entropy_loss': np.mean(entropy_losses), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'policy_gradient_loss': np.mean(pg_losses), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'value_loss': np.mean(value_losses), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'approx_kl': np.mean(approx_kl_divs), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'clip_fraction': np.mean(clip_fractions), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'loss': loss.item(), 'timesteps': self.num_timesteps})\n",
        "        wandb.log({'explained_variance': explained_var, 'timesteps': self.num_timesteps})\n",
        "\n",
        "        # Customizar um grafico para as acoes no tempo\n",
        "        acoes = actions.tolist()\n",
        "        wandb.log({'acoes':  np.mean(acoes), \n",
        "                   'timesteps': self.num_timesteps, \n",
        "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
        "                   'value_loss': np.mean(value_losses), \n",
        "                   'loss': loss.item()\n",
        "                   }\n",
        "        )\n",
        "        \n",
        "        wandb.log({'Yard':  (Yard.cont/Yard.space)*100, \n",
        "                   'timesteps': self.num_timesteps, \n",
        "                   'mean_reward_test': safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),\n",
        "                   'value_loss': np.mean(value_losses), \n",
        "                   'loss': loss.item()\n",
        "                   }\n",
        "        )\n",
        "        # --------- WandB Log ----------- #\n",
        "\n",
        "\n",
        "    def learn(\n",
        "        self: SelfRecurrentPPO,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 1,\n",
        "        tb_log_name: str = \"RecurrentPPO\",\n",
        "        reset_num_timesteps: bool = True,\n",
        "        progress_bar: bool = False,\n",
        "    ) -> SelfRecurrentPPO:\n",
        "        iteration = 0\n",
        "\n",
        "        total_timesteps, callback = self._setup_learn(\n",
        "            total_timesteps,\n",
        "            callback,\n",
        "            reset_num_timesteps,\n",
        "            tb_log_name,\n",
        "            progress_bar,\n",
        "        )\n",
        "\n",
        "        callback.on_training_start(locals(), globals())\n",
        "\n",
        "        while self.num_timesteps < total_timesteps:\n",
        "            continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n",
        "\n",
        "            if continue_training is False:\n",
        "                break\n",
        "\n",
        "            iteration += 1\n",
        "            self._update_current_progress_remaining(self.num_timesteps, total_timesteps)\n",
        "\n",
        "            # Display training infos\n",
        "            if log_interval is not None and iteration % log_interval == 0:\n",
        "                time_elapsed = max((time.time_ns() - self.start_time) / 1e9, sys.float_info.epsilon)\n",
        "                fps = int((self.num_timesteps - self._num_timesteps_at_start) / time_elapsed)\n",
        "                self.logger.record(\"time/iterations\", iteration, exclude=\"tensorboard\")\n",
        "                if len(self.ep_info_buffer) > 0 and len(self.ep_info_buffer[0]) > 0:\n",
        "                    self.logger.record(\"rollout/ep_rew_mean\", safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]))\n",
        "                    self.logger.record(\"rollout/ep_len_mean\", safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]))\n",
        "                    # --------- WandB Log ----------- #\n",
        "                    wandb.log({\"mean_reward_test\": safe_mean([ep_info[\"r\"] for ep_info in self.ep_info_buffer]),'timesteps': self.num_timesteps})\n",
        "                    wandb.log({\"ep_len_mean\": safe_mean([ep_info[\"l\"] for ep_info in self.ep_info_buffer]),'timesteps': self.num_timesteps})\n",
        "                self.logger.record(\"time/fps\", fps)\n",
        "                self.logger.record(\"time/time_elapsed\", int(time_elapsed), exclude=\"tensorboard\")\n",
        "                self.logger.record(\"time/total_timesteps\", self.num_timesteps, exclude=\"tensorboard\")\n",
        "                self.logger.dump(step=self.num_timesteps)\n",
        "\n",
        "            self.train()\n",
        "\n",
        "        callback.on_training_end()\n",
        "\n",
        "        return self"
      ],
      "metadata": {
        "id": "ycfz7Jb5Tj9q"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OReDpRxVeMSr"
      },
      "source": [
        "## **Ambiente SOMN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIMa_T3pfXiN"
      },
      "source": [
        "### **Classe Yard (*Pátio*)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A56GFQOReRcs"
      },
      "outputs": [],
      "source": [
        "class Yard:\n",
        "    def __init__(self, Y, numFeat, typFeat):\n",
        "        Yard.Y=Y\n",
        "        Yard.yard = [0 for _ in range(numFeat)]\n",
        "        Yard.cont=0\n",
        "        Yard.space = Y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs98_1jtgS_x"
      },
      "source": [
        "### **Classe Demand (*Demanda*)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Hu_K0pqGgZTZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class Demand:\n",
        "\n",
        "    cont=0      # by_frederic ---> mudei de 1 para 0\n",
        "\n",
        "    # Y=20,M=10,N=10,MAXDO=10,MAXAM=5,MAXPR=1.5,MAXPE=50,MAXFT=5,MAXMT=3,MAXTI=2,MAXEU = 30\n",
        "    def __init__(self,M:int,N:int, MAXDO:int, MAXAM:int, MAXPR:float, MAXPE:int, MAXFT:int, MAXMT:int, MAXTI:int, MAXEU:int, t: int):\n",
        "        \n",
        "        Demand.M=M\n",
        "        Demand.N=N\n",
        "        Demand.MAXDO=MAXDO\n",
        "        Demand.MAXAM=MAXAM\n",
        "        Demand.MAXPR=MAXPR\n",
        "        Demand.MAXPE=MAXPE\n",
        "        Demand.MAXFT=MAXFT\n",
        "        Demand.MAXMT=MAXMT\n",
        "        Demand.MAXTI=MAXTI\n",
        "        Demand.MAXEU=MAXEU\n",
        "        Demand.EU = np.random.random(M)*MAXEU\n",
        "        self.ST = int(-1)                  ### free(-1) received(0), ready(1), rejected(2), produced(3), stored(4) and delivered(5)   \n",
        "        Demand.cont +=1\n",
        "  \n",
        "    def __call__(self, t:int):\n",
        "\n",
        "        self.CU = Demand.cont\n",
        "    #   self.PR = random.randrange(3,Demand.MAXPR)  below -----------------\n",
        "        self.AM = 1 # random.randrange(1,Demand.MAXAM)\n",
        "        self.PE = random.randint(1,Demand.MAXPE)\n",
        "        self.ST = int(0)                  ###received0, ready1, rejected2, produced3, stored4 and delivered5\n",
        "        self.FT = np.random.randint(0,Demand.MAXFT,self.M) \n",
        "        if not np.any(self.FT):\n",
        "            self.FT[0] = 1      ## by_frederic\n",
        "\n",
        "        ### Tempo ###   \n",
        "        self.F = 0\n",
        "        for i in range(self.M):\n",
        "            self.F += int(self.FT[i]>0)\n",
        "\n",
        "        self.LT = int(self.F/2) + 2                      ###  --- 1.0*self.fun_tau() * self.F\n",
        "        self.DI = t\n",
        "        self.DO = t + self.LT + random.randint(0,Demand.MAXDO)\n",
        "        \n",
        "        self.CO = 0.0\n",
        "        for j in range(Demand.M):\n",
        "            self.CO += self.FT[j] * Demand.EU[j]\n",
        "        self.CO = self.AM * self.CO\n",
        "    #   self.PR = Demand.MAXPR*self.CO\n",
        "        self.PR = Demand.MAXPE\n",
        "\n",
        "        self.SP = 1.0*self.fun_gamma() ####* Yard.Y   #SPACE CONSUMPTION FACTOR\n",
        "        self.VA = 1.0*self.fun_upsilon() \n",
        "        self.SU = 1.0*self.fun_sigma() \n",
        "        self.TP = self.DO - t\n",
        "\n",
        "\n",
        "    def fun_gamma(self) -> float:\n",
        "        x = (self.AM*self.F)/(Demand.MAXAM * self.M)\n",
        "        return x\n",
        "\n",
        "    def fun_tau(self) -> float:\n",
        "        x = (self.AM*self.F)/(Demand.MAXAM * self.M)\n",
        "        return x\n",
        "\n",
        "    def fun_upsilon(self) -> float:\n",
        "        x = self.F/self.M\n",
        "        return x\n",
        "\n",
        "    def fun_sigma(self) -> float:\n",
        "        x = self.F/self.M\n",
        "        return x\n",
        "  \n",
        "#  def fun_beta(self, IN, OU) -> float:\n",
        "#    x=0\n",
        "#    for i in range(self.M):\n",
        "#      if IN[i]==OU[i]:\n",
        "#        x+=1\n",
        "#    x = x/self.M\n",
        "#    return x\n",
        "     \n",
        "#  def calculate_statics(self):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distribuição de Poisson"
      ],
      "metadata": {
        "id": "R_QeeOWeV48j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import poisson\n",
        "\n",
        "#atraso = poisson.rvs(mu=2, size=100, random_state=0)\n",
        "atraso = [poisson.rvs(mu=5) for _ in range(3000)]\n",
        "sns.histplot(atraso, discrete=True)\n",
        "plt.xlim([-1,10])\n",
        "plt.xlabel('Atraso (k)')\n",
        "plt.ylabel('P(X=k)')\n",
        "#atraso"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HpbeLWMa8RDf",
        "outputId": "fb590710-fd75-4738-9d28-0f055e22decf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'P(X=k)')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poisson.rvs(mu=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ic50rcOIrlW",
        "outputId": "4a642ef0-a39d-44cf-d081-1c15d3077cd3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9K2qJ0cgbtR"
      },
      "source": [
        "### **Classe Somn (*Ambiente SOMN*)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BeCsCfJAp-h_"
      },
      "outputs": [],
      "source": [
        "from numpy.random.mtrand import seed\n",
        "# a biblioteca gym mudou\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces  # Discrete, Box, Tuple,  Dict\n",
        "from gymnasium import Env\n",
        "\n",
        "# biblioteca stable_baselines3\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "# from stable_baselines3 import PPO, A2C, DQN, DDPG, TD3\n",
        "\n",
        "# outras bibliotecas\n",
        "import os\n",
        "import numpy as np\n",
        "import random\n",
        "import numpy as np\n",
        "from absl import flags\n",
        "from scipy.stats import poisson\n",
        "import torch\n",
        "import wandb\n",
        "\n",
        "\n",
        "class Somn(Env):\n",
        "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        M: int,\n",
        "        N: int,\n",
        "        Y: int,\n",
        "        MAXDO: int,\n",
        "        MAXAM: int,\n",
        "        MAXPR: int,\n",
        "        MAXPE: int,\n",
        "        MAXFT: int,\n",
        "        MAXMT: int,\n",
        "        MAXTI: int,\n",
        "        MAXEU: int,\n",
        "        #seed: int\n",
        "    ):\n",
        "        super(Somn).__init__()\n",
        "\n",
        "\n",
        "\n",
        "        Somn.time = 1\n",
        "        self.M = M\n",
        "        self.N = N\n",
        "        self.Y = Y\n",
        "        self.MAXDO = MAXDO\n",
        "        self.MAXAM = MAXAM\n",
        "        self.MAXPR = MAXPR\n",
        "        self.MAXPE = MAXPE\n",
        "        self.MAXFT = MAXFT\n",
        "        self.MAXMT = MAXMT\n",
        "        self.MAXTI = MAXTI\n",
        "        self.MAXEU = MAXEU\n",
        "        # self.MT = np.random.randint(0,MAXFT,M)\n",
        "        self.EU = np.random.random(M) * MAXEU\n",
        "        self.BA = np.random.randint(0, MAXFT, M)\n",
        "        self.IN = np.random.randint(0, MAXFT, M)\n",
        "        self.OU = np.random.randint(0, MAXFT, M)\n",
        "        #self.seed = seed\n",
        "        #self.atraso = atraso # (by_frederic)\n",
        "\n",
        "        # self.state = np.zeros((N,5))\n",
        "\n",
        "        # print('Inicializado', M, N , Y)\n",
        "\n",
        "        self.DE = [\n",
        "            Demand(\n",
        "                M, N, MAXDO, MAXAM, MAXPR, MAXPE, MAXFT, MAXMT, MAXTI, MAXEU, Somn.time\n",
        "            )\n",
        "            for _ in range(N)\n",
        "        ]\n",
        "        self.YA = [Yard(Y, M, MAXFT) for _ in range(Y)]\n",
        "\n",
        "        ######################\n",
        "        #      lb e ub       #\n",
        "        ######################\n",
        "        \"\"\"\n",
        "        (lb=lowerbound ub=upperbound) para o espaco de Observacao e Acao \n",
        "        \"\"\"\n",
        "\n",
        "        # ST varia de -2 a 5\n",
        "        self.lb_ST = -2\n",
        "        self.ub_ST = 5\n",
        "        # time varia de 1 a (10*MAXDO + M)\n",
        "        self.lb_time = 1\n",
        "        self.ub_time = 10 * self.MAXDO + self.M\n",
        "        # LT varia de 2 a (M/2 + 2)\n",
        "        self.lb_LT = 2\n",
        "        self.ub_LT = int(self.M / 2) + 2\n",
        "        # DO varia de 3 a (ub_time + ub_LT + MAXDO)\n",
        "        self.lb_DO = 3\n",
        "        self.ub_DO = self.ub_time + self.ub_LT + self.MAXDO\n",
        "        # TP varia de 2 a (ub_time + ub_LT + 2) onde 2 e um ruido\n",
        "        self.lb_TP = 2\n",
        "        self.ub_TP = self.ub_time + self.ub_LT + 2\n",
        "        # MT varia de 0 a MAXFT\n",
        "        self.lb_MT = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
        "        self.ub_MT = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
        "        # EU varia de 0 a MAXEU\n",
        "        self.lb_EU = np.array([0 for _ in range(self.M)]).astype(np.float64)\n",
        "        self.ub_EU = np.array([MAXEU for _ in range(self.M)]).astype(np.float64)\n",
        "        # BA varia de 0 a MAXFT\n",
        "        self.lb_BA = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
        "        self.ub_BA = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
        "        # IN varia de 0 a MAXFT\n",
        "        self.lb_IN = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
        "        self.ub_IN = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
        "        # OU varia de 0 a MAXFT\n",
        "        self.lb_OU = np.array([0 for _ in range(self.M)]).astype(np.int64)\n",
        "        self.ub_OU = np.array([MAXFT for _ in range(self.M)]).astype(np.int64)\n",
        "\n",
        "        # lb e ub--- segunda versao (sem a coluna com os valores de Somn.time)\n",
        "        # self.lb = np.array([[self.lb_ST, self.lb_LT, self.lb_DO, self.lb_TP] for _ in range(self.N)])\n",
        "        # self.ub = np.array([[self.ub_ST, self.ub_LT, self.ub_DO, self.ub_TP] for _ in range(self.N)])\n",
        "\n",
        "        # # lb e ub--- primeira versao (com a coluna com os valores de Somn.time)\n",
        "        # self.lb = np.array([[self.lb_ST, self.lb_time, self.lb_LT, self.lb_DO, self.lb_TP] for _ in range(self.N)])\n",
        "        # self.ub = np.array([[self.ub_ST, self.ub_time, self.ub_LT, self.ub_DO, self.ub_TP] for _ in range(self.N)])\n",
        "\n",
        "        ######################\n",
        "        #      Espacos       #\n",
        "        ######################\n",
        "        \"\"\"\n",
        "        Precisa mudar o espaco de acao \n",
        "        de acordo com o algoritmo utilizado\n",
        "        \"\"\"\n",
        "\n",
        "        # accept to produce or reject\n",
        "        # self.action_space = spaces.Box(0, 4, shape=(1,)) # usar o TD3\n",
        "        self.action_space = spaces.Discrete(self.MAXDO)  # usar com o PPO, DQN, A2C\n",
        "\n",
        "        # Espaco de observacao (como ficam as demandas depois da acao)\n",
        "        # self.observation_space = spaces.Box(self.lb, self.ub, dtype=int)\n",
        "        # self.observation_space = spaces.Dict({'tempo':spaces.Box(self.lb_time, self.ub_time, shape=(1,), dtype=int),\n",
        "        #                                 'estado': spaces.Box(self.lb, self.ub, dtype=int)})      # versao para MultiInputPolicy\n",
        "        # self.observation_space = spaces.Dict({'time':spaces.Box(self.lb_time, self.ub_time, shape=(1,), dtype=int),\n",
        "        #                                'MT': spaces.Box(self.lb_MT, self.ub_MT, dtype=int),\n",
        "        #                                'EU': spaces.Box(self.lb_EU, self.ub_EU, dtype=float),\n",
        "        #                                'BA': spaces.Box(self.lb_BA, self.ub_BA, dtype=int),\n",
        "        #                                'IN': spaces.Box(self.lb_IN, self.ub_IN, dtype=int),\n",
        "        #                                'OU': spaces.Box(self.lb_OU, self.ub_OU, dtype=int),\n",
        "        #                                'state': spaces.Box(self.lb, self.ub, dtype=int)})          # versao para MultiInputPolicy\n",
        "\n",
        "        self.observation_space = spaces.Dict(\n",
        "            {\n",
        "                \"time\": spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float64),\n",
        "                \"MT\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
        "                \"EU\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
        "                \"BA\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
        "                \"IN\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
        "                \"OU\": spaces.Box(low=0.0, high=1.0, shape=(self.M,), dtype=np.float64),\n",
        "                \"state\": spaces.Box(\n",
        "                    low=0.0, high=1.0, shape=(self.N, 4), dtype=np.float64\n",
        "                ),\n",
        "            }\n",
        "        )  # versao para MultiInputPolicy Normalizada\n",
        "\n",
        "    ######################\n",
        "    #      funcoes       #\n",
        "    ######################\n",
        "\n",
        "    def normaliza(self, x, min, max):\n",
        "        x_norm = np.array((x - min) / (max - min)).astype(np.float64)\n",
        "        return x_norm\n",
        "\n",
        "    def readDemand(self):\n",
        "        for i in range(Demand.N):\n",
        "            if (\n",
        "                self.DE[i].ST == -1\n",
        "            ):  # or self.DE[i].ST == 0: ZERO não pode ser status de livre\n",
        "                self.DE[i](Somn.time)\n",
        "\n",
        "    def match_demand_with_inventory(self, limiar: float, t: int) -> bool:\n",
        "        matched = False\n",
        "        for i in range(Demand.N):\n",
        "            for y in range(Yard.cont):\n",
        "                match = 0\n",
        "                # print('Y...', y, 'YA=', YA[y].yard,Yard.cont, 'l=', limiar)\n",
        "                for j in range(Demand.M):\n",
        "                    # print('Y(y,j):', y,j, 'Y x D:', self.YA[y].yard[j],self.DE[i].FT[j], 'cont:', Yard.cont, 'l x m:', limiar, match)\n",
        "                    if self.DE[i].FT[j] > 0:\n",
        "                        if self.DE[i].FT[j] <= self.YA[y].yard[j]:\n",
        "                            match = match + 1\n",
        "                    # se for ZERO então não pode ter a caracteristica\n",
        "                    else:\n",
        "                        if self.YA[y].yard[j] == 0:\n",
        "                            match = match + 1\n",
        "\n",
        "                if match >= limiar:\n",
        "                    # print(\"\\n Match: Casou\", Yard.cont)\n",
        "                    self.YA[y].yard = self.YA[\n",
        "                        Yard.cont - 1\n",
        "                    ].yard  ## apaga o registro de match com o último da lista\n",
        "                    Yard.cont -= 1\n",
        "                    self.DE[i].ST = 3  ## produced status\n",
        "                    matched = True\n",
        "\n",
        "            # print(\"\\n Match: Saiu\", Yard.cont)\n",
        "            return matched\n",
        "\n",
        "    def product_scheduling(self, t: int, action):\n",
        "        for i in range(self.N):\n",
        "            if self.DE[i].ST == 1:\n",
        "                if self.DE[i].DO > (t + self.DE[i].LT + action):\n",
        "                    self.DE[i].ST = 3  ## produced status --- remember to run time for each case\n",
        "                    self.OU -= self.DE[i].FT  ## CONSOME OS RECURSOS\n",
        "                    ################################################\n",
        "                    #                                              #\n",
        "                    #                  atraso                      #\n",
        "                    #                                              #\n",
        "                    ################################################\n",
        "                    self.DE[i].TP = t + self.DE[i].LT + poisson.rvs(mu=2)\n",
        "                    # if self.atraso > 5:\n",
        "                    #     self.DE[i].TP = t + self.DE[i].LT + poisson.rvs(mu=2)\n",
        "                    # else:\n",
        "                    #     self.DE[i].TP = t + self.DE[i].LT + self.atraso  # ruido  --- trocar por distribuição poison --- ou por algo que dependa de AM random.randint(1,Demand.MAXTI)\n",
        "                    #     # print('\\n **** PRODUCED because', self.DE[i].DO, '>', t + self.DE[i].LT + action)\n",
        "                else:\n",
        "                    self.DE[i].ST = 2  ## rejected status\n",
        "                    self.OU -= self.DE[i].FT  ### libera do buffer de produção\n",
        "                    self.BA += self.DE[i].FT  ## devolve para o saldo para os próximos\n",
        "                    # print('\\n **** REJECTED by DO', self.DE[i].DO, ' <= DI+LT+act', t , self.DE[i].LT , action)\n",
        "\n",
        "    def product_destination(self, t: int):\n",
        "        for i in range(Demand.N):\n",
        "            if self.DE[i].ST == 3:\n",
        "                if self.DE[i].TP < t:  ### TP eh resultado de LT(#f) + RAND\n",
        "                    if t < self.DE[i].DO:\n",
        "                        self.DE[i].ST = 5  ## produced status --- remember to run time for each case\n",
        "                        # print(\"\\n Destination: Enviou\", Yard.cont)\n",
        "                    else:\n",
        "                        self.DE[i].ST = 4  ## produced status\n",
        "                        if Yard.cont < Yard.Y - 1:\n",
        "                            self.YA[Yard.cont].yard = self.DE[i].FT\n",
        "                            Yard.cont += 1\n",
        "                            # print(\"\\n Destination: Armazenou no YARD\", Yard.cont)\n",
        "                        else:\n",
        "                            self.DE[i].ST = -2  ## NAO CABE ... REJEITADO COM GERAÇÃO DE LIXO (CASO MAIS GRAVE)\n",
        "\n",
        "    def stock_covers_demand(self, t):\n",
        "        covered = True\n",
        "\n",
        "        for i in range(self.N):\n",
        "            if self.DE[i].ST == 0:\n",
        "                DF = self.BA - self.DE[i].FT\n",
        "\n",
        "                OR = np.array(\n",
        "                    [abs(i) if i < 0 else 0 for i in DF]\n",
        "                )  # O QUE PRECISA SER COMPRADO\n",
        "                # print('\\n ORDER from ', DF, ':', OR)\n",
        "                if not np.any(OR):\n",
        "                    self.DE[i].ST = 1\n",
        "                    self.BA -= np.array(DF)  ### ATUALIZA O SALDO\n",
        "                    self.OU += np.array(DF)  ### ATUALIZA A SAÍDA\n",
        "                    # print('\\n balance:', self.BA,  'because not buying',self.OU)\n",
        "                else:\n",
        "                    covered = False\n",
        "                    self.IN += np.array(OR)  ## ATUALIZA O TOTAL DE COMPRAVEIS\n",
        "                    # print('\\n balance: ', self.BA, 'because buying',OR, 'accumulating', self.IN)\n",
        "        return covered\n",
        "\n",
        "    # def order_raw_material(self, t: int):\n",
        "    # self.IN = [random.randint(0,i) if i > 0 else 0 for i in self.IN]\n",
        "    # return self.IN\n",
        "\n",
        "    def eval_final_states(self) -> float:\n",
        "        totProfit = 0.0\n",
        "        totReward = 0.0\n",
        "        totPenalty = 0.0\n",
        "        for i in range(self.N):\n",
        "            if self.DE[i].ST == 2:\n",
        "                self.DE[i].ST = -1  # LIBERA O ESPAÇO APÓS CONTABILIZADO\n",
        "                totProfit += self.DE[i].AM * self.DE[i].PR\n",
        "                # print('REJECTED vvvvvvvvvvvvvvvvvvvvvvvvvvvv')\n",
        "            if self.DE[i].ST == -2:\n",
        "                totPenalty += self.DE[i].CO\n",
        "                self.DE[i].ST = -1  # LIBERA O ESPAÇO APÓS CONTABILIZADO\n",
        "                # print('PREJUIZO $$$$$$$$$$$$$$$$$$$$$$$$$')\n",
        "            if self.DE[i].ST == 4:\n",
        "                totPenalty += totReward / (\n",
        "                    Yard.space - Yard.cont + 1\n",
        "                )  ### penalidade inversamente proporcional ao espaço remanescente\n",
        "                self.DE[i].ST = -1  # LIBERA O ESPAÇO APÓS CONTABILIZADO\n",
        "                totProfit += self.DE[i].AM * self.DE[i].PR\n",
        "                # print('STORED <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<')\n",
        "            if self.DE[i].ST == 5:\n",
        "                totReward += self.DE[i].AM * self.DE[i].PR\n",
        "                self.DE[i].ST = -1  # LIBERA O ESPAÇO APÓS CONTABILIZADO\n",
        "                totProfit += self.DE[i].AM * self.DE[i].PR\n",
        "                # print('REWARD ******************************')\n",
        "        totReward -= totPenalty\n",
        "        # print ('REW+PEN+PRO', totReward, totPenalty, totProfit)\n",
        "        return totReward, totPenalty, totProfit\n",
        "\n",
        "    ######################\n",
        "    #       step         #\n",
        "    ######################\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        Atualiza tudo aqui e devolve o próximo estado: n_state, reward, done, info\n",
        "\n",
        "            - n_state: próximo estado; \n",
        "            - reward: recompensa da ação; \n",
        "            - done: flag de conclusão; \n",
        "            - info: informaões extras (opcional)\n",
        "\n",
        "        Primeira versão vai fazer uma iteração para cada episódio ... \n",
        "        O Tempo t precisa ser controlado\n",
        "        \"\"\"\n",
        "\n",
        "        # receive RAW MATERIAL AND ORDERS (DEMANDS)\n",
        "        self.MT = np.array([random.randint(0, i) if i > 0 else 0 for i in self.IN])\n",
        "        self.readDemand()\n",
        "\n",
        "        # IF PREVIOUS ORDERS INVENTORY AVAILABLE, PLEASE DISPATCH\n",
        "        if self.match_demand_with_inventory(self.MAXFT / 5, Somn.time):\n",
        "            self.product_destination(Somn.time)\n",
        "\n",
        "        # ANYWAY, UPDATE BALANCE AND INCOME RAW MATERIAL REGARDING MT RECEIVED\n",
        "        self.IN -= self.MT\n",
        "        self.BA += self.MT\n",
        "\n",
        "        # IF RAW MATERIAL INVENTORY DOES NOT COVER PLEASE REQUEST RAW MATERIAL\n",
        "        if not self.stock_covers_demand(Somn.time):\n",
        "            self.IN = np.array(\n",
        "                [random.randint(0, i) if i > 0 else 0 for i in self.IN]\n",
        "            ).astype(np.int64)\n",
        "\n",
        "        # ANYWAY START PRODUCING AND DISPATCHING\n",
        "        self.product_scheduling(Somn.time, action)\n",
        "        self.product_destination(Somn.time)\n",
        "        Somn.time += 1\n",
        "\n",
        "        # ORDINARY PROCEDURES IN STEP METHOD INCLUDING REWARD BY INSPECTING FINAL STATES\n",
        "        # 1 STATE\n",
        "        arrayState = []\n",
        "\n",
        "        for i in range(self.N):\n",
        "            aux_row = [\n",
        "                self.normaliza(x=self.DE[i].ST, min=self.lb_ST, max=self.ub_ST),\n",
        "                # Somn.time,\n",
        "                # self.DE[i].SP,\n",
        "                self.normaliza(self.DE[i].LT, self.lb_LT, self.ub_LT),\n",
        "                # self.DE[i].VA,\n",
        "                # self.DE[i].SU,\n",
        "                # self.DE[i].PR,\n",
        "                self.normaliza(self.DE[i].DO, self.lb_DO, self.ub_DO),\n",
        "                self.normaliza(self.DE[i].TP, self.lb_TP, self.ub_TP),\n",
        "            ]\n",
        "            arrayState.append(aux_row)\n",
        "\n",
        "        self.state = np.array(arrayState)\n",
        "\n",
        "        # 2 REWARD\n",
        "        (\n",
        "            reward,\n",
        "            penalty,\n",
        "            exprofit,\n",
        "        ) = self.eval_final_states()  # aqui vai a função que calcula a recompensa\n",
        "\n",
        "        # Gera grafico do Yard (by_frederic)\n",
        "\n",
        "\n",
        "        # 3 FINAL CONDITION\n",
        "        done = False\n",
        "        truncated = False\n",
        "        # if penalty>0:\n",
        "        # reward =0\n",
        "        # print('\\n D -- O -- N -- E --', self.state)\n",
        "        # done = True\n",
        "\n",
        "        if Somn.time >= self.ub_time:  # 10*Demand.MAXDO + Demand.M   (TEMPOMAX)\n",
        "            # print('\\n D -- O -- N -- E --', self.state)\n",
        "            done = True\n",
        "\n",
        "        # Atualiza o upper bounds\n",
        "\n",
        "        self.ub_MT = max(self.MAXFT, np.amax(self.MT))\n",
        "        self.ub_BA = max(self.MAXFT, np.amax(self.BA))\n",
        "        self.ub_IN = max(self.MAXFT, np.amax(self.IN))\n",
        "\n",
        "        info = {}  # Informações adicionais\n",
        "        # observation = self.state  #by_frederic: retorna quando e um tipo Box\n",
        "        observation = {\n",
        "            \"time\": np.array([self.normaliza(self.time, self.lb_time, self.ub_time)]),\n",
        "            \"MT\": self.normaliza(self.MT, self.lb_MT, self.ub_MT),\n",
        "            \"EU\": self.normaliza(self.EU, self.lb_EU, self.ub_EU),\n",
        "            \"BA\": self.normaliza(self.BA, self.lb_BA, self.ub_BA),\n",
        "            \"IN\": self.normaliza(self.IN, self.lb_IN, self.ub_IN),\n",
        "            \"OU\": self.normaliza(self.OU, self.lb_OU, self.ub_OU),\n",
        "            \"state\": self.state,\n",
        "        }  # by_frederic: retorna quando e um tipo Dict\n",
        "\n",
        "        return (\n",
        "            observation,\n",
        "            reward,\n",
        "            done,\n",
        "            truncated,\n",
        "            info,\n",
        "        )  # , exprofit   # by_frederic:\n",
        "\n",
        "    ######################\n",
        "    #       reset        #\n",
        "    ######################\n",
        "\n",
        "    def reset(self):\n",
        "        #super().reset(seed=seed)\n",
        "        self.MT = np.random.randint(0, self.MAXFT, self.M)\n",
        "        self.EU = np.random.random(self.M) * self.MAXEU\n",
        "        self.BA = np.random.randint(0, self.MAXFT, self.M)\n",
        "        self.IN = np.random.randint(0, self.MAXFT, self.M)\n",
        "        self.OU = np.random.randint(0, self.MAXFT, self.M)\n",
        "        Somn.time = 1\n",
        "\n",
        "        self.YA = [Yard(self.Y, self.M, self.MAXFT) for _ in range(self.Y)]\n",
        "\n",
        "        arrayState = []\n",
        "        for i in range(self.N):\n",
        "            self.DE[i](Somn.time)\n",
        "            aux_row = [\n",
        "                self.normaliza(x=self.DE[i].ST, min=self.lb_ST, max=self.ub_ST),\n",
        "                # Somn.time,\n",
        "                # self.DE[i].SP,\n",
        "                self.normaliza(x=self.DE[i].LT, min=self.lb_LT, max=self.ub_LT),\n",
        "                # self.DE[i].VA,\n",
        "                # self.DE[i].SU,\n",
        "                # self.DE[i].PR,\n",
        "                self.normaliza(x=self.DE[i].DO, min=self.lb_DO, max=self.ub_DO),\n",
        "                self.normaliza(x=self.DE[i].TP, min=self.lb_TP, max=self.ub_TP),\n",
        "            ]\n",
        "            arrayState.append(aux_row)\n",
        "\n",
        "        self.state = np.array(arrayState)\n",
        "\n",
        "        info = dict()\n",
        "        # observation = (self.state, info)  # by_frederic: retorna quando o tipo é Box\n",
        "        observation = {\n",
        "            \"time\": np.array([self.normaliza(self.time, self.lb_time, self.ub_time)]),\n",
        "            \"MT\": self.normaliza(self.MT, self.lb_MT, self.ub_MT),\n",
        "            \"EU\": self.normaliza(self.EU, self.lb_EU, self.ub_EU),\n",
        "            \"BA\": self.normaliza(self.BA, self.lb_BA, self.ub_BA),\n",
        "            \"IN\": self.normaliza(self.IN, self.lb_IN, self.ub_IN),\n",
        "            \"OU\": self.normaliza(self.OU, self.lb_OU, self.ub_OU),\n",
        "            \"state\": self.state,\n",
        "        }  # by_frederic: retorna quando e um tipo Dict\n",
        "\n",
        "        return (observation, info)  # by_frederic: para se adequar ao Gymnasium\n",
        "\n",
        "    ######################\n",
        "    #       render       #\n",
        "    ######################\n",
        "\n",
        "    def render(self):\n",
        "        # print(\"Current state (RENDER): \\n\", self.state)\n",
        "        pass\n",
        "\n",
        "    ######################\n",
        "    #       close        #\n",
        "    ######################\n",
        "\n",
        "    def close(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2thIvoiozDpw"
      },
      "source": [
        "## **Sweep para Hiperparametrização**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g5SoU615veP"
      },
      "source": [
        "### Parametros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WgsToTeSzPJs"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    #'method': 'bayes'\n",
        "    'method': 'random'\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jkQCjmXT1dK2"
      },
      "outputs": [],
      "source": [
        "metric = {\n",
        "    'name': 'mean_reward_test',\n",
        "    'goal': 'maximize'   \n",
        "    }\n",
        "\n",
        "config['metric'] = metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4gwjL3an1lve"
      },
      "outputs": [],
      "source": [
        "parameters_dict = {\n",
        "    'batch_size': {\n",
        "        'values': [16, 32, 64, 128, 256]       \n",
        "      },\n",
        "    # 'clip_range': {\n",
        "    #     'values': [0.1, 0.2, 0.3]\n",
        "    #     },\n",
        "    'ent_coef': {\n",
        "        'distribution': 'uniform',\n",
        "        'max': 0.01,\n",
        "        'min': 0\n",
        "    },\n",
        "    'gae_lambda': {\n",
        "        'distribution': 'uniform',\n",
        "        'max': 1,\n",
        "        'min': 0.9\n",
        "    },\n",
        "    'gamma': {\n",
        "        'distribution': 'uniform',\n",
        "        'max': 1,\n",
        "        'min': 0.8\n",
        "    },\n",
        "    'learning_rate': {\n",
        "        'distribution': 'uniform',\n",
        "        'max': 0.0009,\n",
        "        'min': 1e-05\n",
        "    },\n",
        "    # 'max_grad_norm': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'max': 1,\n",
        "    #     'min': 0.5\n",
        "    # },\n",
        "    'n_epochs': {\n",
        "        'distribution': 'int_uniform',\n",
        "        'max': 30,\n",
        "        'min': 5\n",
        "    },\n",
        "    'n_steps': {\n",
        "        'values': [1024, 1536, 2048, 2304, 2560, 2816, 3072, 3328, 3584, 3840, 4096]\n",
        "    },\n",
        "    # 'stats_window_size': {\n",
        "    #     'distribution': 'int_uniform',\n",
        "    #     'max': 200,\n",
        "    #     'min': 50\n",
        "    # },\n",
        "    'target_kl': {\n",
        "        'distribution': 'uniform',\n",
        "        'max': 0.03,\n",
        "        'min': 0.003\n",
        "    },\n",
        "    # 'total_timesteps': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'max': 2000000,\n",
        "    #     'min': 50000\n",
        "    # },\n",
        "    # 'vf_coef': {\n",
        "    #     'distribution': 'uniform',\n",
        "    #     'max': 1,\n",
        "    #     'min': 0.5\n",
        "    #}\n",
        "}\n",
        "\n",
        "config['parameters'] = parameters_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Print Parametros"
      ],
      "metadata": {
        "id": "EwcqLE7iQeXB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDyHCaY54gxu",
        "outputId": "f0b37edc-af8b-4275-8591-b5ace88d348e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'method': 'random',\n",
            " 'metric': {'goal': 'maximize', 'name': 'mean_reward_test'},\n",
            " 'parameters': {'batch_size': {'values': [16, 32, 64, 128, 256]},\n",
            "                'ent_coef': {'distribution': 'uniform', 'max': 0.01, 'min': 0},\n",
            "                'gae_lambda': {'distribution': 'uniform', 'max': 1, 'min': 0.9},\n",
            "                'gamma': {'distribution': 'uniform', 'max': 1, 'min': 0.8},\n",
            "                'learning_rate': {'distribution': 'uniform',\n",
            "                                  'max': 0.0009,\n",
            "                                  'min': 1e-05},\n",
            "                'n_epochs': {'distribution': 'int_uniform',\n",
            "                             'max': 30,\n",
            "                             'min': 5},\n",
            "                'n_steps': {'values': [1024,\n",
            "                                       1536,\n",
            "                                       2048,\n",
            "                                       2304,\n",
            "                                       2560,\n",
            "                                       2816,\n",
            "                                       3072,\n",
            "                                       3328,\n",
            "                                       3584,\n",
            "                                       3840,\n",
            "                                       4096]},\n",
            "                'target_kl': {'distribution': 'uniform',\n",
            "                              'max': 0.03,\n",
            "                              'min': 0.003}}}\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcj9OZuT5qTv"
      },
      "source": [
        "### Inicializar o Sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5x0YJck5mIg",
        "outputId": "bbc1ca98-0a99-493f-baff-419bd0dbf507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: y4lr9j62\n",
            "Sweep URL: https://wandb.ai/lacmor/Somn_02/sweeps/y4lr9j62\n"
          ]
        }
      ],
      "source": [
        "sweep_id = wandb.sweep(config, project=\"Somn_02\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZRiTLaG7Gmj"
      },
      "source": [
        "### Funcao treinamento1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "obs: criar a pasta dentro do drive /content/drive/MyDrive/SOMN2"
      ],
      "metadata": {
        "id": "2mg4XnxLYT_K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VMFYA87n7Et0"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "#import gym\n",
        "\n",
        "#from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "\n",
        "#from wandb.integration.sb3 import WandbCallback\n",
        "\n",
        "def make_env():\n",
        "    env = Somn(Y=10,M=5,N=7,MAXDO=10,MAXAM=5,MAXPR=1.5,MAXPE=10,MAXFT=5,MAXMT=3,MAXTI=2,MAXEU = 10)\n",
        "    env = Monitor(env)  # record stats such as returns\n",
        "    return env\n",
        "\n",
        "def treinamento1(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    if len(wandb.patched[\"tensorboard\"]) > 0:\n",
        "        wandb.tensorboard.unpatch()\n",
        "    #wandb.tensorboard.patch(root_logdir=\"/content/drive/MyDrive/SOMN2/runs\")\n",
        "    wandb.tensorboard.patch(root_logdir=\"./runs\")\n",
        "    run1 = wandb.init(project='Somn_02', config=config, save_code=True)\n",
        "    #project=\"Somn_01\",\n",
        "    #config=config,\n",
        "    #sync_tensorboard=False,  # auto-upload sb3's tensorboard metrics\n",
        "    #monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    #save_code=True,  # optional\n",
        "    #) as run:\n",
        "    # settings=wandb.Settings(start_method=\"thread\")\n",
        "    # ) as run:\n",
        "\n",
        "    # If called by wandb.agent, as below,\n",
        "    # this config will be set by Sweep Controller\n",
        "    config = wandb.config\n",
        "    env1 = DummyVecEnv([make_env])\n",
        "    \n",
        "\n",
        "    # algoritmos = [PPO, RecurrentPPO]\n",
        "    # politica ={PPO: \"MultiInputPolicy\", RecurrentPPO: \"MlpLstmPolicy\"}\n",
        "    \n",
        "        \n",
        "    model1 = PPO(\n",
        "        policy=\"MultiInputPolicy\", \n",
        "        env=env1, \n",
        "        learning_rate=config.learning_rate, \n",
        "        n_steps=config.n_steps, \n",
        "        batch_size=config.batch_size, \n",
        "        n_epochs=config.n_epochs, \n",
        "        gamma=config.gamma, \n",
        "        gae_lambda=config.gae_lambda, \n",
        "        #clip_range=config.clip_range, \n",
        "        ent_coef=config.ent_coef,\n",
        "        #vf_coef=config.vf_coef, \n",
        "        #max_grad_norm=config.max_grad_norm,\n",
        "        target_kl=config.target_kl,\n",
        "        #stats_window_size=config.stats_window_size, \n",
        "        verbose=0, \n",
        "        #seed = 2023,\n",
        "        device='cpu',\n",
        "        #tensorboard_log=f\"/content/drive/MyDrive/SOMN2/runs/{run1.id}\"\n",
        "        tensorboard_log=f\"runs/{run1.id}\"\n",
        "    )\n",
        "\n",
        "    model1.learn(total_timesteps=1000000)\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Funcao treinamento2"
      ],
      "metadata": {
        "id": "0rtiKdTFX21x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "#import gym\n",
        "\n",
        "#from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv, VecVideoRecorder\n",
        "\n",
        "#from wandb.integration.sb3 import WandbCallback\n",
        "\n",
        "# def make_env():\n",
        "#     env = Somn(Y=10,M=5,N=7,MAXDO=10,MAXAM=5,MAXPR=1.5,MAXPE=10,MAXFT=5,MAXMT=3,MAXTI=2,MAXEU = 10)\n",
        "#     env = Monitor(env)  # record stats such as returns\n",
        "#     return env\n",
        "\n",
        "def treinamento2(config=None):\n",
        "    # Initialize a new wandb run\n",
        "    if len(wandb.patched[\"tensorboard\"]) > 0:\n",
        "        wandb.tensorboard.unpatch()\n",
        "    #wandb.tensorboard.patch(root_logdir=\"/content/drive/MyDrive/SOMN2/runs\")\n",
        "    wandb.tensorboard.patch(root_logdir=\"./runs\")\n",
        "    run2 = wandb.init(project='Somn_02', config=config, save_code=True)\n",
        "    #project=\"Somn_01\",\n",
        "    #config=config,\n",
        "    #sync_tensorboard=False,  # auto-upload sb3's tensorboard metrics\n",
        "    #monitor_gym=True,  # auto-upload the videos of agents playing the game\n",
        "    #save_code=True,  # optional\n",
        "    #) as run:\n",
        "    # settings=wandb.Settings(start_method=\"thread\")\n",
        "    # ) as run:\n",
        "\n",
        "    # If called by wandb.agent, as below,\n",
        "    # this config will be set by Sweep Controller\n",
        "    config = wandb.config\n",
        "    env2 = DummyVecEnv([make_env])\n",
        "    \n",
        "\n",
        "    # algoritmos = [PPO, RecurrentPPO]\n",
        "    # politica ={PPO: \"MultiInputPolicy\", RecurrentPPO: \"MlpLstmPolicy\"}\n",
        "    \n",
        "        \n",
        "\n",
        "    model2 = RecurrentPPO(\n",
        "        policy=\"MultiInputLstmPolicy\", \n",
        "        env=env2, \n",
        "        learning_rate=config.learning_rate, \n",
        "        n_steps=config.n_steps, \n",
        "        batch_size=config.batch_size, \n",
        "        n_epochs=config.n_epochs, \n",
        "        gamma=config.gamma, \n",
        "        gae_lambda=config.gae_lambda, \n",
        "        #clip_range=config.clip_range, \n",
        "        ent_coef=config.ent_coef,\n",
        "        #vf_coef=config.vf_coef, \n",
        "        #max_grad_norm=config.max_grad_norm,\n",
        "        target_kl=config.target_kl,\n",
        "        #stats_window_size=config.stats_window_size, \n",
        "        verbose=0, \n",
        "        #seed = 2023,\n",
        "        device='cpu',\n",
        "        #tensorboard_log=f\"/content/drive/MyDrive/SOMN2/runs/{run2.id}\"\n",
        "        tensorboard_log=f\"runs/{run2.id}\"\n",
        "    )\n",
        "\n",
        "    model2.learn(total_timesteps=1000000)\n",
        "    wandb.finish()\n"
      ],
      "metadata": {
        "id": "qr6YLrBcxXDa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Executar o agente do Sweep"
      ],
      "metadata": {
        "id": "-ZCAWmrZYJg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seed"
      ],
      "metadata": {
        "id": "hNuVgqpoYMJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch as th\n",
        "\n",
        "SEED = 1\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    #os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    #th.manual_seed(seed)\n",
        "    #th.cuda.manual_seed(seed)\n",
        "    #th.backends.cudnn.deterministic = True\n",
        "    #env.seed(seed)\n",
        "\n",
        "##One call at beginning is enough\n",
        "seed_everything(SEED)"
      ],
      "metadata": {
        "id": "uPS0Ulf3A1rB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpLIJN1FAxyJ"
      },
      "source": [
        "#### Iniciando o Agente\n",
        "\n",
        "A célula abaixo lançará um `agente` que executa `treinamento1` e `treinamento2` 20 vezes, usando os valores de hiperparâmetro gerados aleatoriamente retornados pelo controlador de varredura. A execução leva um tempo razoavelmente alto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "4vD2fz9PAxLp",
        "outputId": "ff89b75c-46b8-41d4-e595-79c0e6df109f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zwhnohni with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tent_coef: 0.009497584254891256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tgae_lambda: 0.927337330229357\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tgamma: 0.8488446882953791\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000769800720496922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_epochs: 12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_steps: 2304\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttarget_kl: 0.010035496336149224\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230510_200431-zwhnohni</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/lacmor/Somn_02/runs/zwhnohni' target=\"_blank\">polished-sweep-1</a></strong> to <a href='https://wandb.ai/lacmor/Somn_02' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/lacmor/Somn_02/sweeps/y4lr9j62' target=\"_blank\">https://wandb.ai/lacmor/Somn_02/sweeps/y4lr9j62</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/lacmor/Somn_02' target=\"_blank\">https://wandb.ai/lacmor/Somn_02</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/lacmor/Somn_02/sweeps/y4lr9j62' target=\"_blank\">https://wandb.ai/lacmor/Somn_02/sweeps/y4lr9j62</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/lacmor/Somn_02/runs/zwhnohni' target=\"_blank\">https://wandb.ai/lacmor/Somn_02/runs/zwhnohni</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.agent(sweep_id, treinamento2, count=20)\n",
        "wandb.agent(sweep_id, treinamento1, count=20)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "zIMa_T3pfXiN",
        "vs98_1jtgS_x",
        "R_QeeOWeV48j",
        "-9K2qJ0cgbtR",
        "8g5SoU615veP"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}